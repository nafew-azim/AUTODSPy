{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "s-ofhMk81JMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U dspy\n",
        "!pip install datasets\n",
        "!pip install torch transformers"
      ],
      "metadata": {
        "id": "8f8USlc81Pk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "puQBhysbF8GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True)"
      ],
      "metadata": {
        "id": "xPYQDQdrGIvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.1:8b"
      ],
      "metadata": {
        "id": "s15REzP7GLVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "pSE1J0Ka1Qp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel  # Change the models to your prefered model\n",
        "import dspy\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from pprint import pprint\n",
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True)\n"
      ],
      "metadata": {
        "id": "jm7kh-8z1VFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure DSPy & LLM**"
      ],
      "metadata": {
        "id": "P2fL54qW1YFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Configure DSPy with Ollama LLM ===\n",
        "lm = dspy.LM('ollama_chat/llama3.1:8b', api_base='http://localhost:11434', api_key='')\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "qChGqSMO1dTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Training Datasets**"
      ],
      "metadata": {
        "id": "LJfufzFc3g37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load GSM8K and HotpotQA for training/testing ===\n",
        "print(\"Loading datasets...\")\n",
        "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train[:10]\")\n",
        "hotpotqa = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train[:10]\", trust_remote_code=True)\n",
        "\n",
        "gsm8k_list = list(gsm8k)\n",
        "hotpotqa_list = list(hotpotqa)\n",
        "\n",
        "# Combine prompts and ground truths\n",
        "data = [(ex[\"question\"], ex[\"answer\"].split(\"####\")[-1].strip()) for ex in gsm8k_list] \\\n",
        "     + [(ex[\"question\"], ex[\"answer\"]) for ex in hotpotqa_list]\n",
        "\n",
        "random.shuffle(data)\n",
        "prompts, ground_truths = zip(*data)\n",
        "\n",
        "prompts = list(prompts)\n",
        "ground_truths = list(ground_truths)\n"
      ],
      "metadata": {
        "id": "ZJFiRemR3i4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action Space & Pool**"
      ],
      "metadata": {
        "id": "lTzQRR2u3rkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define Action Space ===\n",
        "MODULES = [\"CoT\", \"Predict\"]\n",
        "SIGNATURES = [\n",
        "    \"question -> answer\", \"text -> summary\", \"question -> reasoning\", \"question -> hypothesis\",\n",
        "    \"problem -> solution\", \"problem_description -> explanation\", \"context -> summary\", \"context -> briefing\",\n",
        "    \"word_problem -> solution\", \"math_problem -> answer\", \"prompt -> response\", \"query -> response\",\n",
        "    \"text -> response\", \"prompt -> generated_text\", \"query -> generated_text\"\n",
        "]\n",
        "ACTIONS = MODULES + SIGNATURES + [\"stop\"]"
      ],
      "metadata": {
        "id": "i322yPyi35w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy Model Setup**"
      ],
      "metadata": {
        "id": "QNCBtd3b4HNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Initialize Policy Model (GPT-2) ===\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "policy_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_model.to(device)\n",
        "\n",
        "# Try loading saved weights\n",
        "save_path = \"trained_policy_model.pt\"\n",
        "try:\n",
        "    policy_model.load_state_dict(torch.load(save_path))\n",
        "    print(f\"✅ Loaded existing model from {save_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"⚠️ No existing model found, starting fresh...\")\n"
      ],
      "metadata": {
        "id": "BkP1JCTR4G0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "qILppNRW4hig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Answer Comparison Function ===\n",
        "def compare_answers(response, ground_truth):\n",
        "    \"\"\"\n",
        "    Extract final answer from both response and ground truth.\n",
        "    Prefer bracketed content (e.g., [5]), fallback to last number.\n",
        "    Returns 1.0 if they match, else 0.0.\n",
        "    \"\"\"\n",
        "    # Extract from response\n",
        "    response_bracket_matches = re.findall(r'\\[(.*?)\\]', str(response))\n",
        "    if response_bracket_matches:\n",
        "        response_answer = response_bracket_matches[-1].strip()\n",
        "        print(f\"Response bracket match: {response_answer}\")\n",
        "    else:\n",
        "        response_num_matches = re.findall(r'-?\\d*\\.?\\d+', str(response))\n",
        "        response_answer = response_num_matches[-1] if response_num_matches else None\n",
        "        print(f\"Response numeric fallback: {response_answer}\")\n",
        "\n",
        "    # Extract from ground truth\n",
        "    gt_bracket_matches = re.findall(r'\\[(.*?)\\]', str(ground_truth))\n",
        "    if gt_bracket_matches:\n",
        "        gt_answer = gt_bracket_matches[-1].strip()\n",
        "        print(f\"GT bracket match: {gt_answer}\")\n",
        "    else:\n",
        "        gt_num_matches = re.findall(r'-?\\d*\\.?\\d+', str(ground_truth))\n",
        "        gt_answer = gt_num_matches[-1] if gt_num_matches else None\n",
        "        print(f\"GT numeric fallback: {gt_answer}\")\n",
        "\n",
        "    if response_answer and gt_answer:\n",
        "        return 1.0 if response_answer == gt_answer else 0.0\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def chain_response(prompt):\n",
        "    \"\"\"\n",
        "    Processes a prompt through a fixed chain of dspy modules:\n",
        "      1. retrieve: query -> passages\n",
        "      2. cot: question, passages -> reasoning\n",
        "      3. predict: question, reasoning -> answer\n",
        "\n",
        "    Each module's output is passed as input to the next module.\n",
        "    The final answer is returned.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # Module: cot (signature: question, passages -> reasoning)\n",
        "        # We use the original prompt as the question.\n",
        "        if prompt:\n",
        "            response_cot = dspy.ChainOfThought(\"question -> reasoning\")(question=prompt)\n",
        "            reasoning = response_cot.get(\"reasoning\", \"\")\n",
        "        else:\n",
        "            reasoning = \"\"\n",
        "\n",
        "        # Module: predict (signature: reasoning -> answer)\n",
        "        if reasoning:\n",
        "            response_predict = dspy.Predict(\"question, reasoning -> answer\")(question=f\"system instruction: Must give your final answer in square brackets without fail! e.g., [final answer] like this: [5] \\n prompt:{prompt}\", reasoning=reasoning)\n",
        "            answer = response_predict.get(\"answer\", \"\")\n",
        "        else:\n",
        "            answer = \"\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error during processing:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_pipeline(prompt, max_steps=3):\n",
        "    \"\"\"\n",
        "    Generate a discrete DSPy pipeline using a fixed set of actions.\n",
        "    This function restricts the output to a sequence of actions chosen from MODULES, SIGNATURES, and \"stop\".\n",
        "    Note: Each action is approximated by using the logit of its first token.\n",
        "    \"\"\"\n",
        "    state = {\"prompt\": prompt, \"partial_pipeline\": []}\n",
        "    actions_taken = []\n",
        "    log_probs = []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        input_text = f\"Prompt: {state['prompt']} Pipeline: {' '.join(state['partial_pipeline'])}\"\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = policy_model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        if not state[\"partial_pipeline\"]:\n",
        "            valid_actions = MODULES\n",
        "        elif len(state[\"partial_pipeline\"]) == 1 and state[\"partial_pipeline\"][0] in MODULES:\n",
        "            valid_actions = SIGNATURES\n",
        "        elif len(state[\"partial_pipeline\"]) == 2:\n",
        "            valid_actions = [\"stop\"]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        # Map each valid action to a token id (using first token as approximation)\n",
        "        valid_token_ids = []\n",
        "        for a in valid_actions:\n",
        "            tokens = tokenizer.encode(a, add_special_tokens=False)\n",
        "            if len(tokens) == 0:\n",
        "                continue\n",
        "            valid_token_ids.append(tokens[0])\n",
        "\n",
        "        # Get logits and sample one action\n",
        "        action_logits = logits[0, valid_token_ids]\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "        action_choice = torch.multinomial(action_probs, 1).item()\n",
        "        chosen_token_id = valid_token_ids[action_choice]\n",
        "        action = valid_actions[action_choice]\n",
        "        log_prob = torch.log(action_probs[action_choice])\n",
        "\n",
        "        actions_taken.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "        state[\"partial_pipeline\"].append(action)\n",
        "\n",
        "        if action == \"stop\":\n",
        "            break\n",
        "\n",
        "    return state[\"partial_pipeline\"], actions_taken, log_probs\n",
        "\n",
        "def execute_pipeline(prompt, pipeline):\n",
        "    \"\"\"\n",
        "    Execute the DSPy pipeline using the dspy modules.\n",
        "    The pipeline should be a list of two actions: [module, signature] (with an optional \"stop\" appended).\n",
        "    \"\"\"\n",
        "    if len(pipeline) != 2 or pipeline[0] not in MODULES or pipeline[1] not in SIGNATURES:\n",
        "        print(\"Invalid pipeline format\")\n",
        "        return None\n",
        "\n",
        "    module, signature = pipeline\n",
        "\n",
        "    if signature.count(\"->\") != 1:\n",
        "        print(\"Signature does not have proper format\")\n",
        "        return None\n",
        "\n",
        "    match = re.match(r\"\\s*([a-zA-Z_ ]+)\\s*->\\s*([a-zA-Z_ ]+)\\s*\", signature)\n",
        "    if not match:\n",
        "        print(\"Regex match failed for signature\")\n",
        "        return None\n",
        "\n",
        "    inputfield = match.group(1).strip().replace(\" \", \"_\").lower()\n",
        "    outputfield = match.group(2).strip().replace(\" \", \"_\").lower()\n",
        "\n",
        "    try:\n",
        "        if module == \"CoT\":\n",
        "            program = dspy.ChainOfThought(signature)\n",
        "        else:  # \"Predict\"\n",
        "            program = dspy.Predict(signature)\n",
        "        response = program(**{inputfield: f\"system instruction: Must give your final answer in square brackets without fail! e.g., [final answer] like this: [5] \\n prompt: {prompt}\"})\n",
        "        fixedPipeLineResponse = chain_response(prompt)\n",
        "        print(\"fixed: \", fixedPipeLineResponse)\n",
        "        return response.get(outputfield), fixedPipeLineResponse\n",
        "    except Exception as e:\n",
        "        print(f\"Execution pipeline failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def compute_reward(prompt, response, fixedPipeLineResponse, ground_truth):\n",
        "    \"\"\"\n",
        "    Computes a reward for the response.\n",
        "    First, a DSPy-format bonus is given if the response strictly matches the desired format.\n",
        "    Then, it attempts to extract the final answer using regex (preferring bracketed content).\n",
        "    The reward is based on matching the numeric (or string) content with the ground truth.\n",
        "    \"\"\"\n",
        "    if response is None:\n",
        "        print(\"Response is None\")\n",
        "        return 0.0\n",
        "\n",
        "    regex1 = compare_answers(response, ground_truth)\n",
        "    regex2 = compare_answers(fixedPipeLineResponse, ground_truth)\n",
        "    print(\"regex1: \", regex1)\n",
        "    print(\"regex2: \", regex2)\n",
        "\n",
        " # If regex score1 is 0, fall back to LLM judge\n",
        "    if regex1 == 0.0:\n",
        "        eval_prompt = f\"\"\"\n",
        "        Evaluate whether the following response correctly answers the prompt based on the ground truth and give your final score in the square brackets[final score]. only the value in the [] should in your response and nothing else.\n",
        "        Prompt: {prompt}\n",
        "        Response: {response}\n",
        "        Ground Truth: {ground_truth}\n",
        "        Return a score in range between 1.0 to 0.0 if the response is correct or partially correct (matches or is equivalent to the ground truth), or 0.0 if incorrect.\n",
        "        final score:[]\n",
        "            \"\"\"\n",
        "        try:\n",
        "            llm_response = lm(eval_prompt)\n",
        "            print(\"LLM Response: \", llm_response)\n",
        "            response_bracket_matches = re.findall(r'\\[(.*?)\\]', (llm_response[0]))\n",
        "            print(\"llm matches: \", response_bracket_matches)\n",
        "            if response_bracket_matches:\n",
        "                response_answer = response_bracket_matches[-1].strip()  # Take the last bracketed content\n",
        "                print(f\"llm Response bracket match: {response_answer}\")\n",
        "            else:\n",
        "                # Fallback: Extract the last numeric value if no brackets\n",
        "                response_num_matches = re.findall(r'-?\\d*\\.?\\d+', (llm_response[0]))\n",
        "                response_answer = response_num_matches[-1] if response_num_matches else None\n",
        "                print(f\"llm Response last numeric match: {response_answer}\")\n",
        "            score_str = response_answer\n",
        "            score = float(score_str.strip())\n",
        "            print(f\"LLM score: {score_str}, in float: {score}\")\n",
        "            # time.sleep(20)  # Wait 20 seconds after LLM call\n",
        "            return min(max(score, 0.0), 1.0)\n",
        "\n",
        "        except (ValueError, TypeError, IndexError):\n",
        "            print(\"LLM evaluation failed, defaulting to 0.0\")\n",
        "            return 0.0\n",
        "    # If regex2 is 0, fall back to LLM judge\n",
        "    if regex2 == 0.0:\n",
        "        eval_prompt = f\"\"\"\n",
        "        Evaluate whether the following response correctly answers the prompt based on the ground truth and give your final score in the square brackets[final score]. only the value in the [] should in your response and nothing else.\n",
        "        Prompt: {prompt}\n",
        "        Response: {fixedPipeLineResponse}\n",
        "        Ground Truth: {ground_truth}\n",
        "        Return a score in range between 1.0 to 0.0 if the response is correct or partially correct (matches or is equivalent to the ground truth), or 0.0 if incorrect.\n",
        "        final score:[]\n",
        "            \"\"\"\n",
        "        try:\n",
        "            llm_response2 = lm(eval_prompt)\n",
        "            print(\"LLM Response2: \", llm_response2)\n",
        "            response_bracket_matches = re.findall(r'\\[(.*?)\\]', (llm_response2[0]))\n",
        "            print(\"llm matches2: \", response_bracket_matches)\n",
        "            if response_bracket_matches:\n",
        "                response_answer = response_bracket_matches[-1].strip()  # Take the last bracketed content\n",
        "                print(f\"llm Response bracket match2: {response_answer}\")\n",
        "            else:\n",
        "                # Fallback: Extract the last numeric value if no brackets\n",
        "                response_num_matches = re.findall(r'-?\\d*\\.?\\d+', (llm_response2[0]))\n",
        "                response_answer = response_num_matches[-1] if response_num_matches else None\n",
        "                print(f\"llm Response last numeric match2: {response_answer}\")\n",
        "            score_str = response_answer\n",
        "            score = float(score_str.strip())\n",
        "            print(f\"LLM score2: {score_str}, in float: {score}\")\n",
        "            # time.sleep(20)  # Wait 20 seconds after LLM call\n",
        "            return min(max(score, 0.0), 1.0)\n",
        "\n",
        "        except (ValueError, TypeError, IndexError):\n",
        "            print(\"LLM evaluation failed, defaulting to 0.0\")\n",
        "            return 0.0\n",
        "    return (max(regex1,regex2))\n",
        "\n",
        "\n",
        "# Run Test models\n",
        "def test_model(trained_model, test_prompt):\n",
        "    trained_model.eval()\n",
        "    with torch.no_grad():\n",
        "        pipeline, actions, _ = generate_pipeline(test_prompt)\n",
        "        response = execute_pipeline(test_prompt, pipeline[:-1] if pipeline[-1]==\"stop\" else pipeline)\n",
        "        print(\"=== Test Result ===\")\n",
        "        print(f\"Prompt: {test_prompt}\")\n",
        "        print(f\"Pipeline: {pipeline}\")\n",
        "        print(f\"Actions: {actions}\")\n",
        "        print(f\"Response: {response}\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "ylfkJ8sW4okJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model**"
      ],
      "metadata": {
        "id": "dE5tihpG40fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rl(num_episodes=500, learning_rate=1e-4, save_path=\"trained_policy_model.pt\"):\n",
        "    \"\"\"\n",
        "    Train the policy model with REINFORCE.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
        "    rewards, losses = [], []\n",
        "    baseline, baseline_alpha = 0.0, 0.9\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        idx = np.random.randint(len(prompts))\n",
        "        prompt, ground_truth = prompts[idx], ground_truths[idx]\n",
        "\n",
        "        pipeline, actions, log_probs = generate_pipeline(prompt)\n",
        "        response, fixedPipeLineResponse = execute_pipeline(prompt, pipeline[:-1] if pipeline[-1]==\"stop\" else pipeline)\n",
        "\n",
        "        reward = compute_reward(prompt, response, fixedPipeLineResponse, ground_truth)\n",
        "\n",
        "        # REINFORCE update\n",
        "        baseline = baseline_alpha * baseline + (1 - baseline_alpha) * reward\n",
        "        advantage = reward - baseline\n",
        "\n",
        "        loss = -sum([lp * advantage for lp in log_probs])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        rewards.append(reward)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        print(f\"Episode {episode+1}/{num_episodes} | Reward={reward:.3f}, Loss={loss.item():.3f}, Pipeline={pipeline}\")\n",
        "\n",
        "    torch.save(policy_model.state_dict(), save_path)\n",
        "    print(f\"✅ Model saved to {save_path}\")\n",
        "    return policy_model, rewards, losses\n"
      ],
      "metadata": {
        "id": "zoCJP1rc47KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Training**"
      ],
      "metadata": {
        "id": "vGUvUzWr5Jjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the parameters to suit your needs\n",
        "trained_model, rewards, losses = train_rl(num_episodes=5, learning_rate=2e-5)\n"
      ],
      "metadata": {
        "id": "_hAhj_oa5YaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Testing Dataset**"
      ],
      "metadata": {
        "id": "90hnYzKRE6L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load test split of datasets ===\n",
        "gsm8k_test = load_dataset(\"gsm8k\", \"main\", split=\"test[:20]\")\n",
        "hotpotqa_test = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train[20:40]\", trust_remote_code=True)\n",
        "\n",
        "gsm8k_test_data = [dspy.Example(\n",
        "    question=ex['question'],\n",
        "    answer=ex[\"answer\"].split(\"####\")[-1].strip(),\n",
        "    task_type='math').with_inputs('question')\n",
        "    for ex in gsm8k_test]\n",
        "\n",
        "hotpotqa_test_data = [dspy.Example(\n",
        "    question=ex['question'],\n",
        "    answer=ex['answer'],\n",
        "    context=ex.get(\"context\",\"\"),\n",
        "    task_type='qa').with_inputs('question')\n",
        "    for ex in hotpotqa_test]\n",
        "\n",
        "print(f\"GSM8K test: {len(gsm8k_test_data)} | HotpotQA test: {len(hotpotqa_test_data)}\")\n"
      ],
      "metadata": {
        "id": "DMEhPnxbFLzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Model**"
      ],
      "metadata": {
        "id": "5Vxq0LYS486u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_module(dataset, model, query=False, output_file=\"evaluation_results.json\"):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on a dataset and save results to JSON.\n",
        "    \"\"\"\n",
        "    total_correct = 0\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"Query mode:\", query)\n",
        "\n",
        "    for i, example in enumerate(dataset):\n",
        "        print(f\"------ Iteration {i} ------\")\n",
        "        prompt = f\"{example.context}\\n\\n{example.question}\" if query else example.question\n",
        "        prediction_text = test_model(model, prompt)\n",
        "\n",
        "        # direct comparison\n",
        "        is_correct = prediction_text and example.answer and \\\n",
        "                     (prediction_text.strip().lower() == example.answer.strip().lower() or\n",
        "                      example.answer.strip().lower() in prediction_text.strip().lower())\n",
        "\n",
        "        score = 1 if is_correct else 0\n",
        "\n",
        "        if not is_correct:  # fallback evaluation using LLM\n",
        "            eval_prompt = f\"\"\"\n",
        "            Evaluate if the response correctly answers the prompt based on the ground truth.\n",
        "            Return [1] if correct, [0] if incorrect.\n",
        "\n",
        "            Prompt: {example.question}\n",
        "            Response: {prediction_text}\n",
        "            Ground Truth: {example.answer}\n",
        "            Final score: []\n",
        "            \"\"\"\n",
        "            llm_res = lm(eval_prompt)\n",
        "            score = 1 if \"1\" in llm_res[0] else 0\n",
        "\n",
        "        total_correct += score\n",
        "\n",
        "        result = {\n",
        "            \"question\": example.question,\n",
        "            \"context\": example.context if query else \"N/A\",\n",
        "            \"response\": prediction_text,\n",
        "            \"ground_truth\": example.answer,\n",
        "            \"score\": score\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Save results\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"✅ Evaluation complete. Total correct = {total_correct}/{len(dataset)}\")\n",
        "    return total_time, total_correct\n"
      ],
      "metadata": {
        "id": "HeVSRRki5D-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Testing**"
      ],
      "metadata": {
        "id": "WHi_obGPFkxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on HotpotQA test subset\n",
        "total_time, total_correct = evaluate_module(hotpotqa_test_data, trained_model, query=True)\n",
        "print(f\"Evaluation time: {total_time:.2f}s | Correct: {total_correct}\")\n"
      ],
      "metadata": {
        "id": "_ehKPkMw5eZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on GSM8K test subset\n",
        "total_time, total_correct = evaluate_module(gsm8k_test_data, trained_model, query=True)\n",
        "print(f\"Evaluation time: {total_time:.2f}s | Correct: {total_correct}\")\n"
      ],
      "metadata": {
        "id": "hNtnTw-GO0YT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}