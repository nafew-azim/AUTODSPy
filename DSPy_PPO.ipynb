{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Dependencies**"
      ],
      "metadata": {
        "id": "LQrgS9rILOMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08zmAPwkLM6n"
      },
      "outputs": [],
      "source": [
        "!pip install -U dspy\n",
        "!pip install datasets\n",
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Ollama**"
      ],
      "metadata": {
        "id": "VtB9rtzeLUb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "93ectrkMLcGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True)"
      ],
      "metadata": {
        "id": "mNoWAafuLdXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.1:8b"
      ],
      "metadata": {
        "id": "o8N_ZRFTLf63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "5gnDbhpILht1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel  # Change the models to your prefered model\n",
        "import dspy\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from pprint import pprint\n",
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True)\n"
      ],
      "metadata": {
        "id": "e-FiIk-cLnn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure DSPy & LLM**"
      ],
      "metadata": {
        "id": "YOwa_tYpLpos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Configure DSPy with Ollama LLM ===\n",
        "lm = dspy.LM('ollama_chat/llama3.1:8b', api_base='http://localhost:11434', api_key='')\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "ePnZwZG9LsG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Training Datasets**"
      ],
      "metadata": {
        "id": "Gt4QQYupL0C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load GSM8K and HotpotQA for training/testing ===\n",
        "print(\"Loading datasets...\")\n",
        "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"train[:10]\")\n",
        "hotpotqa = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train[:10]\", trust_remote_code=True)\n",
        "\n",
        "gsm8k_list = list(gsm8k)\n",
        "hotpotqa_list = list(hotpotqa)\n",
        "\n",
        "# Combine prompts and ground truths\n",
        "data = [(ex[\"question\"], ex[\"answer\"].split(\"####\")[-1].strip()) for ex in gsm8k_list] \\\n",
        "     + [(ex[\"question\"], ex[\"answer\"]) for ex in hotpotqa_list]\n",
        "\n",
        "random.shuffle(data)\n",
        "prompts, ground_truths = zip(*data)\n",
        "\n",
        "prompts = list(prompts)\n",
        "ground_truths = list(ground_truths)\n"
      ],
      "metadata": {
        "id": "pWOs30pWL2c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Action Space & Pool**"
      ],
      "metadata": {
        "id": "dSWpAc9EL8r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define Action Space ===\n",
        "MODULES = [\"CoT\", \"Predict\"]\n",
        "SIGNATURES = [\n",
        "    \"question -> answer\", \"text -> summary\", \"question -> reasoning\", \"question -> hypothesis\",\n",
        "    \"problem -> solution\", \"problem_description -> explanation\", \"context -> summary\", \"context -> briefing\",\n",
        "    \"word_problem -> solution\", \"math_problem -> answer\", \"prompt -> response\", \"query -> response\",\n",
        "    \"text -> response\", \"prompt -> generated_text\", \"query -> generated_text\"\n",
        "]\n",
        "ACTIONS = MODULES + SIGNATURES + [\"stop\"]"
      ],
      "metadata": {
        "id": "b0sge1bIL9Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Policy Model Setup**"
      ],
      "metadata": {
        "id": "fWlGrLOFMAlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "policy_model.to(device)\n",
        "\n",
        "print(\"Policy model loaded\")\n",
        "\n",
        "# ============================================================\n",
        "class ValueHead(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.value = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        return self.value(hidden_states)\n",
        "\n",
        "value_head = ValueHead(policy_model.config.n_embd).to(device)\n",
        "print(\"Value head initialized\")\n",
        "\n",
        "# ============================================================\n",
        "optimizer = Adam(list(policy_model.parameters()) + list(value_head.parameters()), lr=1e-4)\n",
        "print(\"Optimizer initialized\")"
      ],
      "metadata": {
        "id": "Ap_dv2eOMDwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions**"
      ],
      "metadata": {
        "id": "3FZ00kyMMGJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_reward(prompt, response, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute reward based on response accuracy compared to ground truth using LLM.\n",
        "    \"\"\"\n",
        "    if response is None:\n",
        "        return 0.0\n",
        "\n",
        "    eval_prompt = f\"\"\"\n",
        "    Evaluate the correctness of the response compared to the ground truth for the given prompt.\n",
        "    Provide a numerical score between 0.0 and 1.0, where 1.0 means completely correct and 0.0 means completely incorrect.\n",
        "    Return only the numeric score (e.g., 0.75) with no additional text.\n",
        "\n",
        "    Prompt: {prompt}\n",
        "    Response: {response}\n",
        "    Ground Truth: {ground_truth}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        llm_response = lm(eval_prompt)[0].strip()\n",
        "        print(f\"[LLM Judge] Response: '{llm_response}'\")\n",
        "        reward = float(llm_response)\n",
        "        reward = max(0.0, min(1.0, reward))\n",
        "    except ValueError:\n",
        "        print(f\"[LLM Judge] Invalid score format: '{llm_response}'. Defaulting to 0.0\")\n",
        "        reward = 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"[LLM Judge] Error during evaluation: {e}. Defaulting to 0.0\")\n",
        "        reward = 0.0\n",
        "\n",
        "    return reward\n",
        "\n",
        "# ============================================================\n",
        "def execute_pipeline(prompt, pipeline):\n",
        "    \"\"\"\n",
        "    Execute a DSPy pipeline based on the generated sequence.\n",
        "    \"\"\"\n",
        "    if len(pipeline) != 2 or pipeline[0] not in MODULES or pipeline[1] not in SIGNATURES:\n",
        "        print(\"Invalid pipeline format\")\n",
        "        return None\n",
        "\n",
        "    module, signature = pipeline\n",
        "    match = re.match(r\"\\s*([a-zA-Z_ ]+)\\s*->\\s*([a-zA-Z_ ]+)\\s*\", signature)\n",
        "    inputfield = match.group(1).strip().replace(\" \", \"_\").lower()\n",
        "    outputfield = match.group(2).strip().replace(\" \", \"_\").lower()\n",
        "\n",
        "    try:\n",
        "        if module == \"CoT\":\n",
        "            program = dspy.ChainOfThought(signature)\n",
        "        elif module == \"PoT\":\n",
        "            program = dspy.ProgramOfThought(signature)\n",
        "        else:  # \"Predict\"\n",
        "            program = dspy.Predict(signature)\n",
        "\n",
        "        response = program(**{\n",
        "            inputfield: f\"system instruction: Must give your final answer in square brackets without fail! e.g., [final answer] like this: [5] \\n prompt: {prompt}\"\n",
        "        })\n",
        "        return response.get(outputfield)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Execution pipeline failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ============================================================\n",
        "def generate_pipeline_ppo(prompt, max_steps=3):\n",
        "    \"\"\"\n",
        "    Generate a DSPy pipeline using PPO, collecting trajectory data.\n",
        "    \"\"\"\n",
        "    state = {\"prompt\": prompt, \"partial_pipeline\": []}\n",
        "    states, actions, log_probs, values, rewards = [], [], [], [], []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        input_text = f\"Prompt: {state['prompt']} Pipeline: {' '.join(state['partial_pipeline'])}\"\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        outputs = policy_model(**inputs, output_hidden_states=True)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        # Determine valid actions\n",
        "        if not state[\"partial_pipeline\"]:\n",
        "            valid_actions = MODULES\n",
        "        elif len(state[\"partial_pipeline\"]) == 1 and state[\"partial_pipeline\"][0] in MODULES:\n",
        "            valid_actions = SIGNATURES\n",
        "        elif len(state[\"partial_pipeline\"]) == 2:\n",
        "            valid_actions = [\"stop\"]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        # Map actions to token IDs\n",
        "        valid_token_ids = [tokenizer.encode(a, add_special_tokens=False)[0] for a in valid_actions]\n",
        "        action_logits = logits[0, valid_token_ids]\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "\n",
        "        # Sample action\n",
        "        action_idx = torch.multinomial(action_probs, 1).item()\n",
        "        action = valid_actions[action_idx]\n",
        "        log_prob = torch.log(action_probs[action_idx])\n",
        "\n",
        "        # Compute value\n",
        "        hidden_states = outputs.hidden_states[-1][:, -1, :]\n",
        "        value = value_head(hidden_states).item()\n",
        "\n",
        "        # Debug output\n",
        "        print(f\"Step {step+1}:\")\n",
        "        print(f\"  Input: {input_text}\")\n",
        "        print(f\"  Valid Actions: {valid_actions}\")\n",
        "        print(f\"  Selected Action: {action}\")\n",
        "        print(f\"  Log Prob: {log_prob.item():.4f}, Value: {value:.4f}\\n\")\n",
        "\n",
        "        # Store trajectory data\n",
        "        states.append(input_text)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob.detach())\n",
        "        values.append(value)\n",
        "        rewards.append(0.0)\n",
        "\n",
        "        state[\"partial_pipeline\"].append(action)\n",
        "        if action == \"stop\":\n",
        "            break\n",
        "\n",
        "    # Execute pipeline and compute final reward\n",
        "    pipeline = state[\"partial_pipeline\"][:-1] if state[\"partial_pipeline\"] and state[\"partial_pipeline\"][-1] == \"stop\" else state[\"partial_pipeline\"]\n",
        "    response = execute_pipeline(prompt, pipeline)\n",
        "    ground_truth = ground_truths[prompts.index(prompt)]\n",
        "    reward = compute_reward(prompt, response, ground_truth)\n",
        "    rewards[-1] = reward\n",
        "\n",
        "    print(f\"Final Pipeline: {pipeline}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(f\"Computed Reward: {reward}\\n\")\n",
        "\n",
        "    return states, actions, log_probs, rewards, values\n",
        "\n",
        "# ============================================================\n",
        "def compute_gae(rewards, values, next_value, gamma=0.99, lam=0.95):\n",
        "    \"\"\"\n",
        "    Compute Generalized Advantage Estimation (GAE).\n",
        "    \"\"\"\n",
        "    advantages = []\n",
        "    gae = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        if t == len(rewards) - 1:\n",
        "            delta = rewards[t] + gamma * next_value - values[t]\n",
        "        else:\n",
        "            delta = rewards[t] + gamma * values[t + 1] - values[t]\n",
        "        gae = delta + gamma * lam * gae\n",
        "        advantages.insert(0, gae)\n",
        "    return advantages\n",
        "\n",
        "def test_model(trained_model, value_head, test_prompt, max_steps=3):\n",
        "    \"\"\"\n",
        "    Test the trained model by generating and executing a pipeline.\n",
        "    \"\"\"\n",
        "    trained_model.eval()\n",
        "    with torch.no_grad():\n",
        "        state = {\"prompt\": test_prompt, \"partial_pipeline\": []}\n",
        "        for step in range(max_steps):\n",
        "            input_text = f\"Prompt: {state['prompt']} Pipeline: {' '.join(state['partial_pipeline'])}\"\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            outputs = trained_model(**inputs, output_hidden_states=True)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            if not state[\"partial_pipeline\"]:\n",
        "                valid_actions = MODULES\n",
        "            elif len(state[\"partial_pipeline\"]) == 1 and state[\"partial_pipeline\"][0] in MODULES:\n",
        "                valid_actions = SIGNATURES\n",
        "            else:\n",
        "                valid_actions = [\"stop\"]\n",
        "\n",
        "            valid_token_ids = [tokenizer.encode(a, add_special_tokens=False)[0] for a in valid_actions]\n",
        "            action_logits = logits[0, valid_token_ids]\n",
        "            action_probs = torch.softmax(action_logits, dim=-1)\n",
        "            action_idx = torch.argmax(action_probs).item()\n",
        "            action = valid_actions[action_idx]\n",
        "\n",
        "            state[\"partial_pipeline\"].append(action)\n",
        "            print(f\"Test Step {step+1}: Selected Action: {action}\")\n",
        "\n",
        "            if action == \"stop\":\n",
        "                break\n",
        "\n",
        "        pipeline = state[\"partial_pipeline\"][:-1] if state[\"partial_pipeline\"] and state[\"partial_pipeline\"][-1] == \"stop\" else state[\"partial_pipeline\"]\n",
        "        response = execute_pipeline(test_prompt, pipeline)\n",
        "        print(f\"\\nTest Prompt: {test_prompt}\")\n",
        "        print(f\"Generated Pipeline: {pipeline}\")\n",
        "        print(f\"Response: {response}\")\n",
        "        return response\n"
      ],
      "metadata": {
        "id": "AlPNdAXxMK1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model**"
      ],
      "metadata": {
        "id": "WkDmOGE9Mtfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ppo(num_episodes=500, clip_eps=0.2, epochs_per_batch=10, batch_size=10):\n",
        "    \"\"\"\n",
        "    Train the policy model using PPO with GAE and batching.\n",
        "    \"\"\"\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        trajectories = []\n",
        "        print(f\"\\n=== Episode {episode + 1}/{num_episodes} ===\")\n",
        "        for _ in range(batch_size):\n",
        "            idx = np.random.randint(len(prompts))\n",
        "            prompt = prompts[idx]\n",
        "            traj = generate_pipeline_ppo(prompt)\n",
        "            trajectories.append(traj)\n",
        "\n",
        "        # Compute advantages and returns\n",
        "        all_states, all_actions, all_log_probs, all_advantages, all_returns = [], [], [], [], []\n",
        "        for traj in trajectories:\n",
        "            states, actions, log_probs, rewards, values = traj\n",
        "            advantages = compute_gae(rewards, values, next_value=0)\n",
        "            returns = [adv + val for adv, val in zip(advantages, values)]\n",
        "            all_states.extend(states)\n",
        "            all_actions.extend(actions)\n",
        "            all_log_probs.extend(log_probs)\n",
        "            all_advantages.extend(advantages)\n",
        "            all_returns.extend(returns)\n",
        "\n",
        "        # Convert to tensors\n",
        "        all_log_probs = torch.stack(all_log_probs)\n",
        "        all_advantages = torch.tensor(all_advantages, dtype=torch.float32).to(device)\n",
        "        all_returns = torch.tensor(all_returns, dtype=torch.float32).to(device)\n",
        "\n",
        "        # PPO update\n",
        "        for epoch in range(epochs_per_batch):\n",
        "            for i, (state, action) in enumerate(zip(all_states, all_actions)):\n",
        "                inputs = tokenizer(state, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "                outputs = policy_model(**inputs, output_hidden_states=True)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                # Determine valid actions\n",
        "                partial_pipeline = state.split(\"Pipeline: \")[1].split() if \"Pipeline: \" in state else []\n",
        "                if not partial_pipeline:\n",
        "                    valid_actions = MODULES\n",
        "                elif len(partial_pipeline) == 1 and partial_pipeline[0] in MODULES:\n",
        "                    valid_actions = SIGNATURES\n",
        "                else:\n",
        "                    valid_actions = [\"stop\"]\n",
        "\n",
        "                valid_token_ids = [tokenizer.encode(a, add_special_tokens=False)[0] for a in valid_actions]\n",
        "                action_logits = logits[0, valid_token_ids]\n",
        "                new_probs = torch.softmax(action_logits, dim=-1)\n",
        "\n",
        "                try:\n",
        "                    action_idx = valid_actions.index(action)\n",
        "                except ValueError:\n",
        "                    print(f\"Action '{action}' not found in valid actions: {valid_actions}\")\n",
        "                    continue\n",
        "\n",
        "                new_log_prob = torch.log(new_probs[action_idx] + 1e-10)\n",
        "\n",
        "                # PPO clipped objective\n",
        "                ratio = torch.exp(new_log_prob - all_log_probs[i])\n",
        "                surr1 = ratio * all_advantages[i]\n",
        "                surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * all_advantages[i]\n",
        "                policy_loss = -torch.min(surr1, surr2)\n",
        "\n",
        "                # Value loss\n",
        "                hidden_states = outputs.hidden_states[-1][:, -1, :]\n",
        "                new_value = value_head(hidden_states).squeeze()\n",
        "                value_loss = F.mse_loss(new_value, all_returns[i])\n",
        "\n",
        "                # Entropy bonus\n",
        "                entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-10))\n",
        "\n",
        "                # Total loss\n",
        "                loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Debug output\n",
        "                if i % 5 == 0:\n",
        "                    print(f\"Epoch {epoch+1}, Update {i+1}: Loss = {loss.item():.4f}, Policy Loss = {policy_loss.item():.4f}, Value Loss = {value_loss.item():.4f}, Entropy = {entropy.item():.4f}\")\n",
        "\n",
        "        avg_reward = np.mean([traj[3][-1] for traj in trajectories])\n",
        "        rewards_history.append(avg_reward)\n",
        "        print(f\"Episode {episode + 1} Average Reward: {avg_reward:.3f}\")\n",
        "\n",
        "    torch.save(policy_model.state_dict(), \"ppo_policy_model.pt\")\n",
        "    torch.save(value_head.state_dict(), \"ppo_value_head.pt\")\n",
        "    return policy_model, value_head"
      ],
      "metadata": {
        "id": "NCVYEkjUM37w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Training**"
      ],
      "metadata": {
        "id": "UOj3ZCVRM5DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, trained_value_head = train_ppo(num_episodes=20, clip_eps=0.2, epochs_per_batch=10, batch_size=10)"
      ],
      "metadata": {
        "id": "mqf2F0f3NRuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Model**"
      ],
      "metadata": {
        "id": "1LW9P40CNSSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_module(dataset, fulldataset, model, trained_value_head, query=False):\n",
        "    \"\"\"\n",
        "    Evaluates the given module on a dataset of examples.\n",
        "    \"\"\"\n",
        "    total_correct = 0\n",
        "    total_pra_score = 0\n",
        "    results = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"Query mode:\", query)\n",
        "\n",
        "    for i, example in enumerate(dataset):\n",
        "        print(f\"-------------------------------------DATASET NUMBER : {i+1}------------------------------------------------\")\n",
        "\n",
        "        prompt = f\"{example.context} \\n\\n {example.question} \" if query else example.question\n",
        "        prediction_text = test_model(model, trained_value_head, prompt)\n",
        "\n",
        "        # Check correctness\n",
        "        if example.answer:\n",
        "            is_correct = (\n",
        "                prediction_text and prediction_text.strip().lower() == example.answer.strip().lower()\n",
        "            ) or (\n",
        "                prediction_text and example.answer.strip().lower() in prediction_text.strip().lower()\n",
        "            )\n",
        "        else:\n",
        "            is_correct = False\n",
        "\n",
        "        score = 1 if is_correct else 0\n",
        "\n",
        "        # LLM evaluation if not correct\n",
        "        if not is_correct:\n",
        "            eval_prompt = f\"\"\"\n",
        "                Evaluate whether the following response correctly answers the prompt based on the ground truth.\n",
        "                Return a score of 1.0 or 0.0 if the response is correct and 0 if incorrect.\n",
        "                Only return the score inside the square brackets [].\n",
        "\n",
        "                Prompt: {example.question}\n",
        "                Response: {prediction_text}\n",
        "                Ground Truth: {example.answer}\n",
        "                Final score:[]\"\"\"\n",
        "\n",
        "            llm_res = lm(eval_prompt)\n",
        "            score = 1 if \"1\" in llm_res[0] else 0\n",
        "\n",
        "        total_correct += score\n",
        "\n",
        "        print(\"Ground Truth: \", example.answer)\n",
        "        print(\"Score: \", score)\n",
        "\n",
        "        # Store result\n",
        "        if query:\n",
        "            result = {\n",
        "                \"question\": example.question,\n",
        "                \"context\": example.context,\n",
        "                \"response\": prediction_text,\n",
        "                \"ground_truth\": example.answer if example.answer else \"N/A\",\n",
        "                \"score\": score,\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    return total_time, total_correct"
      ],
      "metadata": {
        "id": "fKAWXA3bNYYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Test Datasets**"
      ],
      "metadata": {
        "id": "fgyLRbGzOb1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_test = load_dataset(\"gsm8k\", \"main\", split=\"test[0:20]\")\n",
        "hotpotqa_test = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"train[20:40]\", trust_remote_code=True)\n",
        "\n",
        "gsm8k_test_data = [dspy.Example(question=ex['question'], answer=ex[\"answer\"].split(\"####\")[-1].strip(), task_type='math').with_inputs('question') for ex in gsm8k_test]\n",
        "hotpotqa_test_data = [dspy.Example(question=ex['question'], answer=ex['answer'], context=ex[\"context\"], task_type='qa').with_inputs('question') for ex in hotpotqa_test]\n"
      ],
      "metadata": {
        "id": "gu1tcjoBOgZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Testing**"
      ],
      "metadata": {
        "id": "urDe6YAQOtS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Testing HotpotQA\n",
        "htotal_time, htotal_correct = evaluate_module(hotpotqa_test_data, hotpotqa_test, policy_model, trained_value_head, query=True)\n",
        "print(f\"HotPotQA - Total time: {htotal_time}, Total correct: {htotal_correct}\")"
      ],
      "metadata": {
        "id": "XK1RVS2TOkBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing GSM8K\n",
        "gtotal_time, gtotal_correct = evaluate_module(gsm8k_test_data, gsm8k_test, policy_model, trained_value_head)\n",
        "print(f\"GSM8K - Total time: {gtotal_time}, Total correct: {gtotal_correct}\")"
      ],
      "metadata": {
        "id": "Qq7UxDhXPB5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
